{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12121e87",
   "metadata": {},
   "source": [
    "We'll at least start out with the given guys before feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb34d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.feature_selection import RFECV #was hoping for this without the reverse, but I don't see it. Oh well when\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#it's just 8; well, it'll be more hectic momentarily...\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import root_mean_squared_log_error #Phew this on is on there... I mean find if we'd have to square,\n",
    "#but rather not have to write the formula.\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "# from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6652f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5348b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6150016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following is in case I need it later. As, it seems quite barbaric to sort through the list each one... however this would require\n",
    "#changing the numbers into a dataframe.\n",
    "\n",
    "def log_prepper(num):\n",
    "    return min(0, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bea582ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual custom rmsle....\n",
    "\n",
    "#Ideally, I'd rather not have to actually write the formula out... \n",
    "#Unfortunately this didn't work.Unsure what I did wrong,but eh...\n",
    "\n",
    "def cus_rmsle(actuals, preds):\n",
    "    rmsle = 0\n",
    "    for i in range(len(actuals)):\n",
    "        rmsle += np.log(1+min(preds[i], 0)) - np.log(1+actuals[i])\n",
    "    rmsle = rmsle/len(preds) #Hmm, /+?\n",
    "    return rmsle**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00137fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yeah, after thinking about things the scoring should affect patterns detected, so instead of r2 let's finally change things...\n",
    "\n",
    "scoring = make_scorer(root_mean_squared_log_error, greater_is_better=False)\n",
    "# scoring = make_scorer(mean_squared_log_error, greater_is_better=False, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca94d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90609, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>whole_weight1</th>\n",
       "      <th>whole_weight2</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>rings</th>\n",
       "      <th>sex_I</th>\n",
       "      <th>sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.7715</td>\n",
       "      <td>0.3285</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.630</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.145</td>\n",
       "      <td>1.1300</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>0.2765</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.160</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.555</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>0.3695</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  diameter  height  whole_weight  whole_weight1  whole_weight2  \\\n",
       "0   0.550     0.430   0.150        0.7715         0.3285         0.1465   \n",
       "1   0.630     0.490   0.145        1.1300         0.4580         0.2765   \n",
       "2   0.160     0.110   0.025        0.0210         0.0055         0.0030   \n",
       "3   0.595     0.475   0.150        0.9145         0.3755         0.2055   \n",
       "4   0.555     0.425   0.130        0.7820         0.3695         0.1600   \n",
       "\n",
       "   shell_weight  rings  sex_I  sex_M  \n",
       "0        0.2400     11      0      0  \n",
       "1        0.3200     11      0      0  \n",
       "2        0.0050      6      1      0  \n",
       "3        0.2500     10      0      1  \n",
       "4        0.1975      9      1      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned_train.csv')\n",
    "\n",
    "df = pd.get_dummies(df, columns=['sex'], dtype=int, drop_first=True)\n",
    "#I honestly forgot to earlier, although I do like the data is preserved: At least at the moment we also need to dummify sex\n",
    "#to get our lovely sklearn methods to work:\n",
    "\n",
    "print(df.shape)\n",
    "df.head()#Note that the baseline, ie 0s in the rest, will be indicativ of female abalones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02b1e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('rings', axis=1)\n",
    "y = df['rings']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=26) #Relatively large test size as we\n",
    "#have so much data to work with...\n",
    "#Re. 26 - I like that number. Discussion not for now\n",
    "\n",
    "mms = MinMaxScaler() #Temp. removed to have an easier time seeing which feature names were used.\n",
    "X_train = mms.fit_transform(X_train)\n",
    "X_test = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33a4b8",
   "metadata": {},
   "source": [
    "Given that this is somewhat of a professional environment, as opposed to my boot camp I just finished, I won't bother to explain all of my thoughts. However, note that I'll reuse this and similar lines of code until finding desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad8ff610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_defaults.get_feature_names_out()\n",
    "\n",
    "#Hmm, quite interesting that we chose all the weights.... ie I'd understand if one, but\n",
    "#apparetly the relative discremencies upon focusing on less and less of the alabone, quite literally, is that useful.\n",
    "\n",
    "#Also, the distinguishment between female is somehwat negligible, despite hte many difference we obsered... however from\n",
    "#female to on-binary remains significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a505a",
   "metadata": {},
   "source": [
    "### Temp:\n",
    "\n",
    "I'm going back and trying to get the custom scorer going... it finally is clear to me that the issue I had then is what I had a while back and what I have no (which in short means): Seemingly those records whose predictions would be lower than 0 cause issues even in the fitting stage... Which makes sense as we're already taking the rmsle in the get-go, using htat to try to determine the model... likely can get around it in the linear regression stage, but to use rfe.... and for forests...\n",
    "\n",
    "So, it is incumbent upon us to get around this.\n",
    "\n",
    "### Conclusion re. Temp:\n",
    "\n",
    "Ironic the name of hte Stackoverflow user that commented this - StupidWolf: Anyways, to focus on doing log transformations also to the target. So, I went ahead and did that on modeling_ii and it worked... Ie once the target is also log scaled then it's basically not going to happen to ever get a negative prediction... so we got this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c6357c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bwolf\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:666: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared logarithmic error, use the function'root_mean_squared_log_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m      4\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFECV(lr, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      5\u001b[0m             ,scoring\u001b[38;5;241m=\u001b[39mscoring\n\u001b[0;32m      6\u001b[0m             ,min_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m model_defaults \u001b[38;5;241m=\u001b[39m rfe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     10\u001b[0m preds_model_defaults \u001b[38;5;241m=\u001b[39m rfe\u001b[38;5;241m.\u001b[39mpredict(X_test)\u001b[38;5;66;03m#And we're somehow getting some negative predictions.... note that againt his is\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#without any feature engineering.... Regardless, we'll just brute force it. Unsure if we have a better wayt han the following:\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_selection\\_rfe.py:746\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    743\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    744\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m--> 746\u001b[0m scores \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    747\u001b[0m     func(rfe, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, X, y, train, test, scorer)\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    749\u001b[0m )\n\u001b[0;32m    751\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    752\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_selection\\_rfe.py:747\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    743\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    744\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m    746\u001b[0m scores \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m--> 747\u001b[0m     func(rfe, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, X, y, train, test, scorer)\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    749\u001b[0m )\n\u001b[0;32m    751\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    752\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_selection\\_rfe.py:35\u001b[0m, in \u001b[0;36m_rfe_single_fit\u001b[1;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[0;32m     33\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[0;32m     34\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rfe\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m     36\u001b[0m     X_train,\n\u001b[0;32m     37\u001b[0m     y_train,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m estimator, features: _score(\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;66;03m# TODO(SLEP6): pass score_params here\u001b[39;00m\n\u001b[0;32m     40\u001b[0m         estimator,\n\u001b[0;32m     41\u001b[0m         X_test[:, features],\n\u001b[0;32m     42\u001b[0m         y_test,\n\u001b[0;32m     43\u001b[0m         scorer,\n\u001b[0;32m     44\u001b[0m         score_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     45\u001b[0m     ),\n\u001b[0;32m     46\u001b[0m )\u001b[38;5;241m.\u001b[39mscores_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_selection\\_rfe.py:325\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Compute step score on the previous selection iteration\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# because 'estimator' must use features\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# that have not been eliminated yet\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_score:\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores_\u001b[38;5;241m.\u001b[39mappend(step_score(estimator, features))\n\u001b[0;32m    326\u001b[0m support_[features[ranks][:threshold]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    327\u001b[0m ranking_[np\u001b[38;5;241m.\u001b[39mlogical_not(support_)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_selection\\_rfe.py:38\u001b[0m, in \u001b[0;36m_rfe_single_fit.<locals>.<lambda>\u001b[1;34m(estimator, features)\u001b[0m\n\u001b[0;32m     33\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[0;32m     34\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rfe\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m     36\u001b[0m     X_train,\n\u001b[0;32m     37\u001b[0m     y_train,\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m estimator, features: _score(\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;66;03m# TODO(SLEP6): pass score_params here\u001b[39;00m\n\u001b[0;32m     40\u001b[0m         estimator,\n\u001b[0;32m     41\u001b[0m         X_test[:, features],\n\u001b[0;32m     42\u001b[0m         y_test,\n\u001b[0;32m     43\u001b[0m         scorer,\n\u001b[0;32m     44\u001b[0m         score_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     45\u001b[0m     ),\n\u001b[0;32m     46\u001b[0m )\u001b[38;5;241m.\u001b[39mscores_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:977\u001b[0m, in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, score_params, error_score)\u001b[0m\n\u001b[0;32m    975\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscore_params)\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 977\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, y_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscore_params)\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[0;32m    980\u001b[0m         \u001b[38;5;66;03m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[0;32m    981\u001b[0m         \u001b[38;5;66;03m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_scorer.py:253\u001b[0m, in \u001b[0;36m_BaseScorer.__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     _kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score(partial(_cached_call, \u001b[38;5;28;01mNone\u001b[39;00m), estimator, X, y_true, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_scorer.py:350\u001b[0m, in \u001b[0;36m_Scorer._score\u001b[1;34m(self, method_caller, estimator, X, y_true, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m method_caller(\n\u001b[0;32m    346\u001b[0m     estimator, response_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, X, pos_label\u001b[38;5;241m=\u001b[39mpos_label\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    349\u001b[0m scoring_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sign \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_func(y_true, y_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscoring_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:676\u001b[0m, in \u001b[0;36mmean_squared_log_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    666\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    667\u001b[0m         (\n\u001b[0;32m    668\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msquared\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated in version 1.4 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m     )\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[1;32m--> 676\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_log_error(\n\u001b[0;32m    677\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    678\u001b[0m         )\n\u001b[0;32m    680\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    681\u001b[0m     y_true, y_pred, multioutput\n\u001b[0;32m    682\u001b[0m )\n\u001b[0;32m    683\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:759\u001b[0m, in \u001b[0;36mroot_mean_squared_log_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    756\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (y_true \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m (y_pred \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 759\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    760\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoot Mean Squared Logarithmic Error cannot be used when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    761\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets contain negative values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    762\u001b[0m     )\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    765\u001b[0m     np\u001b[38;5;241m.\u001b[39mlog1p(y_true),\n\u001b[0;32m    766\u001b[0m     np\u001b[38;5;241m.\u001b[39mlog1p(y_pred),\n\u001b[0;32m    767\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    768\u001b[0m     multioutput\u001b[38;5;241m=\u001b[39mmultioutput,\n\u001b[0;32m    769\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values."
     ]
    }
   ],
   "source": [
    "#Oh, just remembered we're supposed to use MSLE. At th least we have a benchmark of what's good 'objectively'\n",
    "\n",
    "lr = LinearRegression()\n",
    "rfe = RFECV(lr, step=1, cv=5\n",
    "            ,min_features_to_select=3)\n",
    "\n",
    "model_defaults = rfe.fit(X_train, y_train)\n",
    "\n",
    "preds_model_defaults = rfe.predict(X_test)#And we're somehow getting some negative predictions.... note that againt his is\n",
    "#without any feature engineering.... Regardless, we'll just brute force it. Unsure if we have a better wayt han the following:\n",
    "\n",
    "for i in range(len(preds_model_defaults)):\n",
    "    if preds_model_defaults[i] < 0:\n",
    "        preds_model_defaults[i] = 0\n",
    "print(preds_model_defaults[preds_model_defaults<0])\n",
    "\n",
    "print(model_defaults.score(X_train, y_train), model_defaults.score(X_test, y_test))\n",
    "print(model_defaults.cv_results_['mean_test_score'])\n",
    "root_mean_squared_log_error(y_test, preds_model_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602a2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4fb8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# []\n",
    "# 0.6062234959016377 0.6034787674182922\n",
    "# [0.59003897 0.59367525 0.59568496 0.59954274 0.59970396 0.60580828\n",
    "#  0.60580744]\n",
    "# 0.16508650876858538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d466f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cf46c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9504620112507841 0.6557858728716266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15345037771109488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A little tempted to play around with the defaults, but eh...consideing that we likely need to engineer some things...\n",
    "\n",
    "rf = RandomForestRegressor(random_state=26)\n",
    "\n",
    "##Seems a bit redundant to use both here...\n",
    "# rfe = RFECV(rf, step=1, cv=5, \n",
    "#                        min_features_to_select=3)\n",
    "\n",
    "model_defaults = rf.fit(X_train, y_train)\n",
    "\n",
    "preds_model_defaults = rf.predict(X_test)\n",
    "\n",
    "for i in range(len(preds_model_defaults)):\n",
    "    if preds_model_defaults[i] < 0:\n",
    "        preds_model_defaults[i] = 0\n",
    "\n",
    "print(model_defaults.score(X_train, y_train), model_defaults.score(X_test, y_test))\n",
    "# print(cross_val_score(rf, \n",
    "#                       X_train, y_train, cv = 5).mean()) #Yeah,c ommenting this guy out... taking too long plus a bit redundant\n",
    "# #Given that random forests are already aggregativing everything\n",
    "root_mean_squared_log_error(y_test, preds_model_defaults)\n",
    "\n",
    "#oooooh yesss this is quite juicy... quite juicy abalane "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cb114e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9313299951392628 0.6238598931844348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16064598506264355"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = BaggingRegressor(random_state=26)\n",
    "\n",
    "##Seems a bit redundant to use both here...\n",
    "# rfe = RFECV(rf, step=1, cv=5, \n",
    "#                        min_features_to_select=3)\n",
    "\n",
    "model_defaults = bag.fit(X_train, y_train)\n",
    "\n",
    "preds_model_defaults = bag.predict(X_test)\n",
    "\n",
    "for i in range(len(preds_model_defaults)):\n",
    "    if preds_model_defaults[i] < 0:\n",
    "        preds_model_defaults[i] = 0\n",
    "\n",
    "print(model_defaults.score(X_train, y_train), model_defaults.score(X_test, y_test))\n",
    "# print(cross_val_score(rf, \n",
    "#                       X_train, y_train, cv = 5).mean()) #Yeah,c ommenting this guy out... taking too long plus a bit redundant\n",
    "# #Given that random forests are already aggregativing everything\n",
    "root_mean_squared_log_error(y_test, preds_model_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d6190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dccc8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;svr&#x27;, SVR())]),\n",
       "             param_grid={&#x27;svr__C&#x27;: [0.9, 0.95, 1], &#x27;svr__kernel&#x27;: [&#x27;rbf&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;svr&#x27;, SVR())]),\n",
       "             param_grid={&#x27;svr__C&#x27;: [0.9, 0.95, 1], &#x27;svr__kernel&#x27;: [&#x27;rbf&#x27;]})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;svr&#x27;, SVR())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SVR<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVR.html\">?<span>Documentation for SVR</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SVR()</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('svr', SVR())]),\n",
       "             param_grid={'svr__C': [0.9, 0.95, 1], 'svr__kernel': ['rbf']})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe = Pipeline([\n",
    "#     ('svr', SVR())\n",
    "# ])\n",
    "# pipe_params = {'svr__kernel' : ['rbf'] #Eh, taking too long to do all of them... just go with rbf for now\n",
    "#                ,'svr__C' : [.9, .95 , 1]\n",
    "#               }\n",
    "# gs = GridSearchCV(pipe,\n",
    "#                   param_grid=pipe_params,\n",
    "#                   cv=5)\n",
    "\n",
    "# gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d06f794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6187870521022931 0.6175574607103471\n",
      "0.6167895215540445\n",
      "{'svr__C': 1, 'svr__kernel': 'rbf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15345037771109488"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds_model_defaults = rf.predict(X_test)\n",
    "# for i in range(len(preds_model_defaults)):\n",
    "#     if preds_model_defaults[i] < 0:\n",
    "#         preds_model_defaults[i] = 0\n",
    "\n",
    "# print(gs.score(X_train, y_train), gs.score(X_test, y_test))\n",
    "# print(gs.best_score_)\n",
    "# print(gs.best_params_)\n",
    "        \n",
    "# root_mean_squared_log_error(y_test, preds_model_defaults)\n",
    "\n",
    "##0.6187870521022931 0.6175574607103471 #Commenting out as it took way too long... note that it has the same RMSLE as forests\n",
    "## 0.6167895215540445\n",
    "## {'svr__C': 1, 'svr__kernel': 'rbf'}\n",
    "## 0.15345037771109488"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff7c0f",
   "metadata": {},
   "source": [
    "So, bottom line we 'like' the accuracy and processing time of Random Forests. Perhaps regression is just that intense, or I'm underappeicating how giant this dataset is, but SVR took forever. So, likely we'll jump just do Forests as we move on. We'll see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c47d93",
   "metadata": {},
   "source": [
    "## The Start of Feature Engineering: Polynomials\n",
    "\n",
    "We'll keep things PG and just go up to squared terms, naturally considering interaction effects as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ccd5289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('rings', axis=1)\n",
    "y = df['rings']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=26)\n",
    "\n",
    "poly = PolynomialFeatures(degree = 2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "df_poly_train = pd.DataFrame(X_train_poly, columns=poly.get_feature_names_out(X.columns)) #We'll use this later\n",
    "df_poly_test = pd.DataFrame(X_test_poly, columns=poly.get_feature_names_out(X.columns))\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X_train = mms.fit_transform(X_train)\n",
    "X_test = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ff7c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poly.get_feature_names_out(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efa6ef60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0.637502643156102 0.6391210169168662\n",
      "[0.61664033 0.61906504 0.62020718 0.62345992 0.62343427 0.62558104\n",
      " 0.62686518 0.6280259  0.62854671 0.62875105 0.62839985 0.62840971\n",
      " 0.6287009  0.62930472 0.62911993 0.62926181 0.62897665 0.62941128\n",
      " 0.62974946 0.62969177 0.6300942  0.63044306 0.63204007 0.63281593\n",
      " 0.6332747  0.63349061 0.6335524  0.63323694 0.633351   0.633187\n",
      " 0.63318795 0.63320335 0.63326952 0.63328497 0.63329516 0.63333263\n",
      " 0.63334052 0.63339293 0.63334014 0.6333579  0.63336914 0.63336319\n",
      " 0.63336625 0.63336738 0.63336738]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1562207342973044"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hmm, so a maximum of 54 features now. let's do this!\n",
    "\n",
    "lr = LinearRegression()\n",
    "rfe = RFECV(lr, step=1, cv=5,\n",
    "                       min_features_to_select=35)\n",
    "\n",
    "model_defaults = rfe.fit(X_train_poly, y_train)\n",
    "\n",
    "preds_model_defaults = rfe.predict(X_test_poly)#And we're somehow getting some negative predictions.... note that againt his is\n",
    "#without any feature engineering.... Regardless, we'll just brute force it. Unsure if we have a better wayt han the following:\n",
    "\n",
    "for i in range(len(preds_model_defaults)):\n",
    "    if preds_model_defaults[i] < 0:\n",
    "        preds_model_defaults[i] = 0\n",
    "print(preds_model_defaults[preds_model_defaults<0])\n",
    "\n",
    "print(model_defaults.score(X_train_poly, y_train), model_defaults.score(X_test_poly, y_test))\n",
    "print(model_defaults.cv_results_['mean_test_score'])\n",
    "root_mean_squared_log_error(y_test, preds_model_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b738753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x1', 'x2', 'x3', 'x4', 'x6', 'x9', 'x10', 'x11', 'x12',\n",
       "       'x13', 'x14', 'x15', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23',\n",
       "       'x26', 'x28', 'x29', 'x30', 'x31', 'x34', 'x35', 'x36', 'x39',\n",
       "       'x40', 'x41', 'x42', 'x44', 'x45', 'x46', 'x48', 'x49'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_defaults.n_features_)\n",
    "model_defaults.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74fe0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Likely some better method exists, but eh...\n",
    "\n",
    "targets = []\n",
    "for i in model_defaults.get_feature_names_out():\n",
    "    targets.append(int(i[1:]))\n",
    "target_columns = []\n",
    "for i in targets:\n",
    "    target_columns.append(df_poly_train.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fca4eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['length',\n",
       " 'diameter',\n",
       " 'height',\n",
       " 'whole_weight',\n",
       " 'whole_weight1',\n",
       " 'shell_weight']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_columns[:6] #Hmm, so on a pure scale we are disregarding the second weight as well as pure sex variables. Howeve,r htey\n",
    "#do manifest in conjecture with others.\n",
    "\n",
    "#Recall earlier how we proposed that inherently, likely, the sex is meaningless and just a proxy for expected weights. So,\n",
    "#perhaps this is inline with what we assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297bb748",
   "metadata": {},
   "source": [
    "Recall that the pure linear rmsle with lr was 0.16508650876858538.\n",
    "\n",
    "Hmm, originally I'm doing this after min/max scaling...yeah that's probably not a move as perhaps the scaling is interfering with their effects. Oh, while all that is true I had a more fundamental mistake - NOOB remember to score around the POLY dataset and not the original one...\n",
    "\n",
    "There we go, at 50 we already have a much lower RMSLE of .156. Let's keep on going for a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e6b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'll leave the name the same, so note how now it's just the subset per the earlier linear regression.\n",
    "\n",
    "X_train_poly = np.array(df_poly_train[target_columns])\n",
    "X_test_poly = np.array(df_poly_test[target_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a719ddfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9500471760962943 0.6566633589144847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15307428574200113"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=26)\n",
    "\n",
    "model_defaults = rf.fit(X_train_poly, y_train)\n",
    "\n",
    "preds_model_defaults = rf.predict(X_test_poly)\n",
    "\n",
    "for i in range(len(preds_model_defaults)):\n",
    "    if preds_model_defaults[i] < 0:\n",
    "        preds_model_defaults[i] = 0\n",
    "\n",
    "print(model_defaults.score(X_train_poly, y_train), model_defaults.score(X_test_poly, y_test))\n",
    "# print(cross_val_score(rf, \n",
    "#                       X_train, y_train, cv = 5).mean()) #Yeah,c ommenting this guy out... taking too long plus a bit redundant\n",
    "# #Given that random forests are already aggregativing everything\n",
    "root_mean_squared_log_error(y_test, preds_model_defaults)\n",
    "\n",
    "#A fair bit lower... Hmm. My intuition tells me to even keep on going for now and get to logs first. So, perhaps\n",
    "#we'll return and actually tune the forest's parameters.\n",
    "\n",
    "#Tempted to do SVR just cause I need to be afk for a while...yeah why not. Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Potential future spot for bagged forests x polys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ea87a6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22653, 36)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_poly = np.array(df_poly_train[target_columns])\n",
    "X_test_poly = np.array(df_poly_test[target_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b71937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6256051790865003 0.6257469825562314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1549904012357784"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr = SVR() #Going with defautls at least for now\n",
    "\n",
    "svr = svr.fit(X_train_poly, y_train)\n",
    "\n",
    "preds_model_defaults = svr.predict(X_test_poly)\n",
    "for i in range(len(preds_model_defaults)):\n",
    "    if preds_model_defaults[i] < 0:\n",
    "        preds_model_defaults[i] = 0\n",
    "\n",
    "print(svr.score(X_train_poly, y_train), svr.score(X_test_poly, y_test))\n",
    "        \n",
    "root_mean_squared_log_error(y_test, preds_model_defaults)\n",
    "\n",
    "#Aww, all that waiting and not even better than the forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fbc4c8",
   "metadata": {},
   "source": [
    "So, we'e made a bit of progress with introducing polynomial degree 2 and interaction effects. While RF is still our best and preferred model, we're down from .15345 to .15307 RMSLE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db13d5",
   "metadata": {},
   "source": [
    "## Round I.5\n",
    "\n",
    "I was on the sofa, relaxing with some William James (ironic that this happeneded in his section on 'Time'), and I recalled that it behooves me to also check if perhaps the features, whether linear or polynomial, possess a stronger/more accurate relationship with rings when rings are squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3c142cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unless I find particular succuess in this, I'll just do linear and forests\n",
    "\n",
    "X = df.drop('rings', axis=1)\n",
    "y = df['rings']**2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=26)\n",
    "\n",
    "poly = PolynomialFeatures(degree = 2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "df_poly_train = pd.DataFrame(X_train_poly, columns=poly.get_feature_names_out(X.columns)) #We'll use this later\n",
    "df_poly_test = pd.DataFrame(X_test_poly, columns=poly.get_feature_names_out(X.columns))\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X_train = mms.fit_transform(X_train)\n",
    "X_test = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a4ac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0.5362205789508885 0.535225347225045\n",
      "[0.53084338 0.53122091 0.53112899 0.53113058 0.53160492 0.53171323\n",
      " 0.53171953 0.5314713  0.53149092 0.53148084 0.53145503 0.53145165\n",
      " 0.53152492 0.53152079 0.53152905 0.53151887 0.53163101 0.53163282\n",
      " 0.53164606 0.53162944 0.53151049 0.53152383 0.53151271 0.53152181\n",
      " 0.53152181]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36388271962439356"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "rfe = RFECV(lr, step=1, cv=5,\n",
    "                       min_features_to_select=30)\n",
    "\n",
    "model_defaults = rfe.fit(X_train_poly, y_train)\n",
    "\n",
    "preds_model_defaults = rfe.predict(X_test_poly)\n",
    "\n",
    "for i in range(len(preds_model_defaults)):\n",
    "    if preds_model_defaults[i] < 0:\n",
    "        preds_model_defaults[i] = 0\n",
    "print(preds_model_defaults[preds_model_defaults<0])\n",
    "\n",
    "print(model_defaults.score(X_train_poly, y_train), model_defaults.score(X_test_poly, y_test))\n",
    "print(model_defaults.cv_results_['mean_test_score'])\n",
    "root_mean_squared_log_error(y_test, preds_model_defaults)\n",
    "\n",
    "#Oh, that's much worse... well, I ain't given' up yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a39caf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "print(model_defaults.n_features_)\n",
    "\n",
    "targets = []\n",
    "for i in model_defaults.get_feature_names_out():\n",
    "    targets.append(int(i[1:]))\n",
    "target_columns = []\n",
    "for i in targets:\n",
    "    target_columns.append(df_poly_train.columns[i])\n",
    "    \n",
    "X_train_poly = np.array(df_poly_train[target_columns])\n",
    "X_test_poly = np.array(df_poly_test[target_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "614d308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9353330249126093 0.5540950426000051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34327315748689535"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=26)\n",
    "\n",
    "model_defaults = rf.fit(X_train_poly, y_train)\n",
    "\n",
    "preds_model_defaults = rf.predict(X_test_poly)\n",
    "\n",
    "for i in range(len(preds_model_defaults)):\n",
    "    if preds_model_defaults[i] < 0:\n",
    "        preds_model_defaults[i] = 0\n",
    "\n",
    "print(model_defaults.score(X_train_poly, y_train), model_defaults.score(X_test_poly, y_test))\n",
    "root_mean_squared_log_error(y_test, preds_model_defaults)\n",
    "\n",
    "#Slightly better than linreg, but eh... still much worse than when assumng rings are linear. Worth a shot though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08bcd1e",
   "metadata": {},
   "source": [
    "## Round II - Logs\n",
    "\n",
    "As we predicted we'd have to do, given assumptions in using a log based metric (first time using such a thing so I honestly don't know if it's grounded anywhere. We'd do it anyways, but still...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029778da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:329: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>whole_weight1</th>\n",
       "      <th>whole_weight2</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>rings</th>\n",
       "      <th>sex_I</th>\n",
       "      <th>sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.597837</td>\n",
       "      <td>-0.843970</td>\n",
       "      <td>-1.897120</td>\n",
       "      <td>-0.259419</td>\n",
       "      <td>-1.113218</td>\n",
       "      <td>-1.920730</td>\n",
       "      <td>-1.427116</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.462035</td>\n",
       "      <td>-0.713350</td>\n",
       "      <td>-1.931022</td>\n",
       "      <td>0.122218</td>\n",
       "      <td>-0.780886</td>\n",
       "      <td>-1.285544</td>\n",
       "      <td>-1.139434</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.832581</td>\n",
       "      <td>-2.207275</td>\n",
       "      <td>-3.688879</td>\n",
       "      <td>-3.863233</td>\n",
       "      <td>-5.203007</td>\n",
       "      <td>-5.809143</td>\n",
       "      <td>-5.298317</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.519194</td>\n",
       "      <td>-0.744440</td>\n",
       "      <td>-1.897120</td>\n",
       "      <td>-0.089378</td>\n",
       "      <td>-0.979497</td>\n",
       "      <td>-1.582309</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.588787</td>\n",
       "      <td>-0.855666</td>\n",
       "      <td>-2.040221</td>\n",
       "      <td>-0.245901</td>\n",
       "      <td>-0.995605</td>\n",
       "      <td>-1.832581</td>\n",
       "      <td>-1.622017</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     length  diameter    height  whole_weight  whole_weight1  whole_weight2  \\\n",
       "0 -0.597837 -0.843970 -1.897120     -0.259419      -1.113218      -1.920730   \n",
       "1 -0.462035 -0.713350 -1.931022      0.122218      -0.780886      -1.285544   \n",
       "2 -1.832581 -2.207275 -3.688879     -3.863233      -5.203007      -5.809143   \n",
       "3 -0.519194 -0.744440 -1.897120     -0.089378      -0.979497      -1.582309   \n",
       "4 -0.588787 -0.855666 -2.040221     -0.245901      -0.995605      -1.832581   \n",
       "\n",
       "   shell_weight     rings  sex_I  sex_M  \n",
       "0     -1.427116  2.397895      0      0  \n",
       "1     -1.139434  2.397895      0      0  \n",
       "2     -5.298317  1.791759      1      0  \n",
       "3     -1.386294  2.302585      0      1  \n",
       "4     -1.622017  2.197225      1      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hmm, we'll start off assuming a simple log of everything. Unsure if that would also include rings...\n",
    "\n",
    "df_log = np.log(df)\n",
    "\n",
    "df_log['sex_I'] = df['sex_I']\n",
    "df_log['sex_M'] = df['sex_M']\n",
    "#Recall that 0 is e to the power of negative infinity; 1 is  to the power of 0... since htese are dummies\n",
    "#let's revert them back to their originals\n",
    "\n",
    "df_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2f63960",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_log.drop('rings', axis=1)\n",
    "y = df_log['rings']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=26)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X_train = mms.fit_transform(X_train)\n",
    "X_test = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a03968d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0.6777817153167525 0.6834449547047344\n",
      "[0.67310332 0.67314658 0.67675627 0.67757212 0.67759681]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05848349109552299"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "#A bit superfl. to have rfe here, but you neverknow... Plus makes the rest of the coding easier.\n",
    "rfe = RFECV(lr, step=1, cv=5,\n",
    "                       min_features_to_select=5)\n",
    "\n",
    "model_defaults = rfe.fit(X_train, y_train)\n",
    "\n",
    "preds_model_defaults = model_defaults.predict(X_test)\n",
    "\n",
    "for i in range(len(preds_model_defaults)):\n",
    "    if preds_model_defaults[i] < 0:\n",
    "        preds_model_defaults[i] = 0\n",
    "print(preds_model_defaults[preds_model_defaults<0])\n",
    "\n",
    "print(model_defaults.score(X_train, y_train), model_defaults.score(X_test, y_test))\n",
    "print(model_defaults.cv_results_['mean_test_score'])\n",
    "root_mean_squared_log_error(y_test, preds_model_defaults)\n",
    "\n",
    "#Awww yeah, that's awesome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b9af91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_defaults.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58a94db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# print(model_defaults.n_features_)\n",
    "\n",
    "# targets = []\n",
    "# for i in model_defaults.get_feature_names_out():\n",
    "#     targets.append(int(i[1:]))\n",
    "# target_columns = []\n",
    "# for i in targets:\n",
    "#     target_columns.append(df_poly_train.columns[i])\n",
    "    \n",
    "# X_train = np.array(df_poly_train[target_columns])\n",
    "# X_test = np.array(df_poly_test[target_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6563d4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595105586184751 0.7214613214990553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05449082576242973"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=26)\n",
    "\n",
    "model_defaults = rf.fit(X_train, y_train)\n",
    "\n",
    "preds_model_defaults = model_defaults.predict(X_test)\n",
    "\n",
    "for i in range(len(preds_model_defaults)):\n",
    "    if preds_model_defaults[i] < 0:\n",
    "        preds_model_defaults[i] = 0\n",
    "\n",
    "print(model_defaults.score(X_train, y_train), model_defaults.score(X_test, y_test))\n",
    "root_mean_squared_log_error(y_test, preds_model_defaults)\n",
    "\n",
    "#Nice, a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8fc8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#While I'm here I'll go ahead and do non-logged rings:\n",
    "X = df_log.drop('rings', axis=1)\n",
    "y = df['rings']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=26)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X_train = mms.fit_transform(X_train)\n",
    "X_test = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49710f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0.5741896897846039 0.5765323834122111\n",
      "[0.56682665 0.56683724 0.5670822  0.57376272 0.57390427]\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20175851656732902"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "#A bit superfl. to have rfe here, but you neverknow... Plus makes the rest of the coding easier.\n",
    "rfe = RFECV(lr, step=1, cv=5,\n",
    "                       min_features_to_select=5)\n",
    "\n",
    "model_defaults = rfe.fit(X_train, y_train)\n",
    "\n",
    "preds_model_defaults = model_defaults.predict(X_test)\n",
    "\n",
    "for i in range(len(preds_model_defaults)):\n",
    "    if preds_model_defaults[i] < 0:\n",
    "        preds_model_defaults[i] = 0\n",
    "print(preds_model_defaults[preds_model_defaults<0])\n",
    "\n",
    "print(model_defaults.score(X_train, y_train), model_defaults.score(X_test, y_test))\n",
    "print(model_defaults.cv_results_['mean_test_score'])\n",
    "print(model_defaults.n_features_)\n",
    "root_mean_squared_log_error(y_test, preds_model_defaults) #Yeah, not nearly as high... Perhaps it's just me being stubborn,\n",
    "#however I don't see why it should inherently be necessary to also natural log the target [just recall that then\n",
    "#the targets, now logged, are targetting the 'default' rings and not logged rings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4442b5a",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "Although much more analysis could be done - ie let's see if perhaps rings shouldn't be natural logged too or perhaps certain features should be polynomialed... this is good enough. Inded, if I'm using the right metric then this blows everybody off the map..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c47ac966",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_model = model_defaults #Scruff when this might get changed a lot... oh well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7105ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60411, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>whole_weight1</th>\n",
       "      <th>whole_weight2</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>sex_I</th>\n",
       "      <th>sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90615</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.155</td>\n",
       "      <td>1.2380</td>\n",
       "      <td>0.6185</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.3005</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90616</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.4785</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90617</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.8395</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.1845</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90618</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.1865</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90619</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.1575</td>\n",
       "      <td>0.0670</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  length  diameter  height  whole_weight  whole_weight1  \\\n",
       "0  90615   0.645     0.475   0.155        1.2380         0.6185   \n",
       "1  90616   0.580     0.460   0.160        0.9830         0.4785   \n",
       "2  90617   0.560     0.420   0.140        0.8395         0.3525   \n",
       "3  90618   0.570     0.490   0.145        0.8740         0.3525   \n",
       "4  90619   0.415     0.325   0.110        0.3580         0.1575   \n",
       "\n",
       "   whole_weight2  shell_weight  sex_I  sex_M  \n",
       "0         0.3125        0.3005      0      1  \n",
       "1         0.2195        0.2750      0      1  \n",
       "2         0.1845        0.2405      0      1  \n",
       "3         0.1865        0.2350      0      1  \n",
       "4         0.0670        0.1050      1      0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned_test.csv')\n",
    "df = pd.get_dummies(df, columns=['sex'], dtype=int, drop_first=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "335924c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:329: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>whole_weight1</th>\n",
       "      <th>whole_weight2</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>sex_I</th>\n",
       "      <th>sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90615</td>\n",
       "      <td>-0.438505</td>\n",
       "      <td>-0.744440</td>\n",
       "      <td>-1.864330</td>\n",
       "      <td>0.213497</td>\n",
       "      <td>-0.480458</td>\n",
       "      <td>-1.163151</td>\n",
       "      <td>-1.202308</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90616</td>\n",
       "      <td>-0.544727</td>\n",
       "      <td>-0.776529</td>\n",
       "      <td>-1.832581</td>\n",
       "      <td>-0.017146</td>\n",
       "      <td>-0.737099</td>\n",
       "      <td>-1.516403</td>\n",
       "      <td>-1.290984</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90617</td>\n",
       "      <td>-0.579818</td>\n",
       "      <td>-0.867501</td>\n",
       "      <td>-1.966113</td>\n",
       "      <td>-0.174949</td>\n",
       "      <td>-1.042705</td>\n",
       "      <td>-1.690106</td>\n",
       "      <td>-1.425035</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90618</td>\n",
       "      <td>-0.562119</td>\n",
       "      <td>-0.713350</td>\n",
       "      <td>-1.931022</td>\n",
       "      <td>-0.134675</td>\n",
       "      <td>-1.042705</td>\n",
       "      <td>-1.679324</td>\n",
       "      <td>-1.448170</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90619</td>\n",
       "      <td>-0.879477</td>\n",
       "      <td>-1.123930</td>\n",
       "      <td>-2.207275</td>\n",
       "      <td>-1.027222</td>\n",
       "      <td>-1.848330</td>\n",
       "      <td>-2.703063</td>\n",
       "      <td>-2.253795</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    length  diameter    height  whole_weight  whole_weight1  \\\n",
       "0  90615 -0.438505 -0.744440 -1.864330      0.213497      -0.480458   \n",
       "1  90616 -0.544727 -0.776529 -1.832581     -0.017146      -0.737099   \n",
       "2  90617 -0.579818 -0.867501 -1.966113     -0.174949      -1.042705   \n",
       "3  90618 -0.562119 -0.713350 -1.931022     -0.134675      -1.042705   \n",
       "4  90619 -0.879477 -1.123930 -2.207275     -1.027222      -1.848330   \n",
       "\n",
       "   whole_weight2  shell_weight  sex_I  sex_M  \n",
       "0      -1.163151     -1.202308      0      1  \n",
       "1      -1.516403     -1.290984      0      1  \n",
       "2      -1.690106     -1.425035      0      1  \n",
       "3      -1.679324     -1.448170      0      1  \n",
       "4      -2.703063     -2.253795      1      0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log = np.log(df)\n",
    "\n",
    "df_log['sex_I'] = df['sex_I']\n",
    "df_log['sex_M'] = df['sex_M']\n",
    "df_log['id'] = df['id']\n",
    "\n",
    "df_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c474e1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>whole_weight1</th>\n",
       "      <th>whole_weight2</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>sex_I</th>\n",
       "      <th>sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60411.000000</td>\n",
       "      <td>60411.000000</td>\n",
       "      <td>60411.000000</td>\n",
       "      <td>6.041100e+04</td>\n",
       "      <td>60411.000000</td>\n",
       "      <td>60411.000000</td>\n",
       "      <td>60411.000000</td>\n",
       "      <td>60411.000000</td>\n",
       "      <td>60411.000000</td>\n",
       "      <td>60411.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>120820.000000</td>\n",
       "      <td>-0.691101</td>\n",
       "      <td>-0.948531</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.487624</td>\n",
       "      <td>-1.339299</td>\n",
       "      <td>-2.037252</td>\n",
       "      <td>-1.731688</td>\n",
       "      <td>0.368161</td>\n",
       "      <td>0.344027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17439.297893</td>\n",
       "      <td>0.270962</td>\n",
       "      <td>0.292039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.840978</td>\n",
       "      <td>0.863855</td>\n",
       "      <td>0.855001</td>\n",
       "      <td>0.827873</td>\n",
       "      <td>0.482309</td>\n",
       "      <td>0.475054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>90615.000000</td>\n",
       "      <td>-2.590267</td>\n",
       "      <td>-2.900422</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-6.214608</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-7.600902</td>\n",
       "      <td>-6.502290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>105717.500000</td>\n",
       "      <td>-0.798508</td>\n",
       "      <td>-1.064211</td>\n",
       "      <td>-2.207275e+00</td>\n",
       "      <td>-0.868692</td>\n",
       "      <td>-1.723167</td>\n",
       "      <td>-2.447611</td>\n",
       "      <td>-2.120264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>120820.000000</td>\n",
       "      <td>-0.606969</td>\n",
       "      <td>-0.855666</td>\n",
       "      <td>-1.966113e+00</td>\n",
       "      <td>-0.221270</td>\n",
       "      <td>-1.111698</td>\n",
       "      <td>-1.795767</td>\n",
       "      <td>-1.491655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>135922.500000</td>\n",
       "      <td>-0.510826</td>\n",
       "      <td>-0.755023</td>\n",
       "      <td>-1.832581e+00</td>\n",
       "      <td>0.067659</td>\n",
       "      <td>-0.766794</td>\n",
       "      <td>-1.458865</td>\n",
       "      <td>-1.187444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>151025.000000</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>-0.430783</td>\n",
       "      <td>9.075436e-02</td>\n",
       "      <td>1.038685</td>\n",
       "      <td>0.397433</td>\n",
       "      <td>-0.443946</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id        length      diameter        height  whole_weight  \\\n",
       "count   60411.000000  60411.000000  60411.000000  6.041100e+04  60411.000000   \n",
       "mean   120820.000000     -0.691101     -0.948531          -inf     -0.487624   \n",
       "std     17439.297893      0.270962      0.292039           NaN      0.840978   \n",
       "min     90615.000000     -2.590267     -2.900422          -inf     -6.214608   \n",
       "25%    105717.500000     -0.798508     -1.064211 -2.207275e+00     -0.868692   \n",
       "50%    120820.000000     -0.606969     -0.855666 -1.966113e+00     -0.221270   \n",
       "75%    135922.500000     -0.510826     -0.755023 -1.832581e+00      0.067659   \n",
       "max    151025.000000     -0.223144     -0.430783  9.075436e-02      1.038685   \n",
       "\n",
       "       whole_weight1  whole_weight2  shell_weight         sex_I         sex_M  \n",
       "count   60411.000000   60411.000000  60411.000000  60411.000000  60411.000000  \n",
       "mean       -1.339299      -2.037252     -1.731688      0.368161      0.344027  \n",
       "std         0.863855       0.855001      0.827873      0.482309      0.475054  \n",
       "min        -6.907755      -7.600902     -6.502290      0.000000      0.000000  \n",
       "25%        -1.723167      -2.447611     -2.120264      0.000000      0.000000  \n",
       "50%        -1.111698      -1.795767     -1.491655      0.000000      0.000000  \n",
       "75%        -0.766794      -1.458865     -1.187444      1.000000      1.000000  \n",
       "max         0.397433      -0.443946      0.003992      1.000000      1.000000  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0535c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_log.drop('id', axis=1) #Eh,t his bit seems a bit scruff to me, but eh...\n",
    "\n",
    "X_val = mms.transform(X)\n",
    "\n",
    "# preds = submission_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c78b6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90615</td>\n",
       "      <td>10.316769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90616</td>\n",
       "      <td>9.571289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90617</td>\n",
       "      <td>10.187230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90618</td>\n",
       "      <td>11.315985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90619</td>\n",
       "      <td>7.552523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      Rings\n",
       "0  90615  10.316769\n",
       "1  90616   9.571289\n",
       "2  90617  10.187230\n",
       "3  90618  11.315985\n",
       "4  90619   7.552523"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(df['id'])\n",
    "submission['Rings'] = np.exp(submission_model.predict(X_val))\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "429cfae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90615</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90616</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90617</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90618</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90619</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  Rings\n",
       "0  90615   10.0\n",
       "1  90616   10.0\n",
       "2  90617   10.0\n",
       "3  90618   11.0\n",
       "4  90619    8.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submission['Rings'] = submission['Rings'].round(0) #Might as well round... these guys are whole numbers so... pragmatically\n",
    "# #might be better to not bother, as we'll be going up and down... oh well. Let's see what happens\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9746b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\n",
    "    '../data/submission_ii.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0af9b",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "\n",
    "Aww, when I submtted i [rounded] I only got .154ish; ii gave me .151ish. Mind you, this is only based upon 20% of the data, however the question remains - did I use the right metric? Furthermore, the models were only optimized on a raw r2 basis... perhaps things would change if the scoring metric was formerly rmsle.\n",
    "\n",
    "Additionally, we could still experiment with not natural logging rings, using a fushion of polynomial and log features. Also, we can actually prune the forest (hyperparameter tune it). Furthermore, how about we stop complaining and use SVR???\n",
    "\n",
    "And finally, for good measure, I'll repeat myself - did I use the right metric?!!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40466ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_clipped = np.clip(y_pred, a_min=0, a_max=None) #Should likely be using this code I found among somebody on Kaggle instead of that loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78192f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_features = [f for f in num_cols if (train[f] >= 0).all() and scipy.stats.skew(train[f]) > 0]\n",
    "# log_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965ed39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec0200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5ad77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some notes in case I need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7408dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([\n",
    "#     ('cvec', CountVectorizer(lowercase=False)),\n",
    "#     ('bc', BaggingClassifier(random_state=26))\n",
    "# ])\n",
    "# pipe_params = {'cvec__min_df' : [0.0]\n",
    "#                ,'cvec__max_df' : [.95]\n",
    "#                ,'cvec__ngram_range' : [(1,2)]\n",
    "#                ,'cvec__max_features' : [1800, 1750, 1700]\n",
    "#                ,'cvec__stop_words' : ['english']\n",
    "#               }\n",
    "# gs = GridSearchCV(pipe,\n",
    "#                   param_grid=pipe_params,\n",
    "#                   cv=5)\n",
    "\n",
    "# gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e11cdfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gs.score(X_train, y_train), gs.score(X_test, y_test))\n",
    "# print(gs.best_score_)\n",
    "# print(gs.best_params_)\n",
    "# #Oh wow, this did a lot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620808d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
