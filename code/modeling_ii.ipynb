{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e510060",
   "metadata": {},
   "source": [
    "Besides other reflections, which I'll copy below this for good measure, maybe let's make things simple, well, relatively, and see how we do when we make gendered models... we saw how vastly different each were, plus would be relatively easy..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3efb3",
   "metadata": {},
   "source": [
    "\"Aww, when I submtted i [rounded] I only got .154ish; ii gave me .151ish. Mind you, this is only based upon 20% of the data, however the question remains - did I use the right metric? Furthermore, the models were only optimized on a raw r2 basis... perhaps things would change if the scoring metric was formerly rmsle.\n",
    "\n",
    "Additionally, we could still experiment with not natural logging rings, using a fushion of polynomial and log features. Also, we can actually prune the forest (hyperparameter tune it). Furthermore, how about we stop complaining and use SVR???\n",
    "\n",
    "And finally, for good measure, I'll repeat myself - did I use the right metric?!!?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed3978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.feature_selection import RFECV #was hoping for this without the reverse, but I don't see it. Oh well when\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#it's just 8; well, it'll be more hectic momentarily...\n",
    "\n",
    "# from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import root_mean_squared_log_error #Phew this on is on there... I mean find if we'd have to square,\n",
    "#but rather not have to write the formula.\n",
    "\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80f47c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90609, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>whole_weight1</th>\n",
       "      <th>whole_weight2</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>rings</th>\n",
       "      <th>sex_I</th>\n",
       "      <th>sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.7715</td>\n",
       "      <td>0.3285</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.630</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.145</td>\n",
       "      <td>1.1300</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>0.2765</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.160</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.555</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>0.3695</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  diameter  height  whole_weight  whole_weight1  whole_weight2  \\\n",
       "0   0.550     0.430   0.150        0.7715         0.3285         0.1465   \n",
       "1   0.630     0.490   0.145        1.1300         0.4580         0.2765   \n",
       "2   0.160     0.110   0.025        0.0210         0.0055         0.0030   \n",
       "3   0.595     0.475   0.150        0.9145         0.3755         0.2055   \n",
       "4   0.555     0.425   0.130        0.7820         0.3695         0.1600   \n",
       "\n",
       "   shell_weight  rings  sex_I  sex_M  \n",
       "0        0.2400     11      0      0  \n",
       "1        0.3200     11      0      0  \n",
       "2        0.0050      6      1      0  \n",
       "3        0.2500     10      0      1  \n",
       "4        0.1975      9      1      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned_train.csv')\n",
    "df = pd.get_dummies(df, columns=['sex'], dtype=int, drop_first=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f82d550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:329: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#I'll jump to logged modeling, as that did the best:\n",
    "\n",
    "df_log = np.log(df)\n",
    "\n",
    "df_log['sex_I'] = df['sex_I'] #Eh, for reference...\n",
    "df_log['sex_M'] = df['sex_M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc8aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "males_log = df_log[df_log['sex_M']==1] #We'll start with males and if this does super well then we'll bother for the rest.\n",
    "ind_log = df_log[df_log['sex_I']==1]\n",
    "females_log = df_log[(df_log['sex_M']==0) & (df_log['sex_I']==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42015322",
   "metadata": {},
   "source": [
    "After our pilot on males, with a mere logreg giving us already .052, we're comitted to gendering the abalones. However, from a coding standpoint what can we do?\n",
    "\n",
    "A bit tempted to brute-force it when there are just three of them...<br>\n",
    "Another option, which I've done in the past, is to make loops to generate the variable names... but that just seems too arduous... So, brute force it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d04bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m = males_log.drop(['rings', 'sex_I', 'sex_M'], axis=1)\n",
    "y_m = males_log['rings']\n",
    "\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_m, y_m, test_size=.25, random_state=26)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X_train_m = mms.fit_transform(X_train_m)\n",
    "X_test_m = mms.transform(X_test_m)\n",
    "\n",
    "###########################################\n",
    "\n",
    "X_f = females_log.drop(['rings', 'sex_I', 'sex_M'], axis=1)\n",
    "y_f = females_log['rings']\n",
    "\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_f, y_f, test_size=.25, random_state=26)\n",
    "\n",
    "mms = MinMaxScaler() #Redubbing in case\n",
    "X_train_f = mms.fit_transform(X_train_f)\n",
    "X_test_f = mms.transform(X_test_f)\n",
    "\n",
    "####################################\n",
    "\n",
    "X_i = ind_log.drop(['rings', 'sex_I', 'sex_M'], axis=1)\n",
    "y_i = ind_log['rings']\n",
    "\n",
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_i, y_i, test_size=.25, random_state=26)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X_train_i = mms.fit_transform(X_train_i)\n",
    "X_test_i = mms.transform(X_test_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "601bccd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0.5161445500137307 0.5239455437966991\n",
      "[0.51380703 0.51480088 0.51478273]\n",
      "0.05214827372018365 is the rmsle of males\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[]\n",
      "0.44644442376205973 0.45985686884031995\n",
      "[0.44410487 0.44625746 0.44619736]\n",
      "0.053150820863495034 is the rmsle of females\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[]\n",
      "0.6451731789442452 0.6453981930754247\n",
      "[0.64445896 0.64444582 0.64465798]\n",
      "0.06037852303767514 is the rmsle of indetermined\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "#And now the moment of truth!\n",
    "\n",
    "current_gender = \"males\"\n",
    "lr = LinearRegression()\n",
    "\n",
    "rfe = RFECV(lr, step=1, cv=5,\n",
    "                       min_features_to_select=5)\n",
    "\n",
    "model_defaults_m = rfe.fit(X_train_m, y_train_m)\n",
    "\n",
    "preds_model_defaults_m = model_defaults_m.predict(X_test_m)\n",
    "\n",
    "for i in range(len(preds_model_defaults_m)):\n",
    "    if preds_model_defaults_m[i] < 0:\n",
    "        preds_model_defaults_m[i] = 0\n",
    "print(preds_model_defaults_m[preds_model_defaults_m<0])\n",
    "\n",
    "print(model_defaults_m.score(X_train_m, y_train_m), model_defaults_m.score(X_test_m, y_test_m))\n",
    "print(model_defaults_m.cv_results_['mean_test_score'])\n",
    "print(f\"{root_mean_squared_log_error(y_test_m, preds_model_defaults_m)} is the rmsle of {current_gender}\")\n",
    "print(f\"~\"*26)\n",
    "#Oh baby, .052 with just linreg!!! Yeah, let's do this...\n",
    "\n",
    "###########################################\n",
    "\n",
    "current_gender = \"females\"\n",
    "lr = LinearRegression()\n",
    "\n",
    "rfe = RFECV(lr, step=1, cv=5,\n",
    "                       min_features_to_select=5)\n",
    "\n",
    "model_defaults_f = rfe.fit(X_train_f, y_train_f)\n",
    "\n",
    "preds_model_defaults_f = model_defaults_f.predict(X_test_f)\n",
    "\n",
    "for i in range(len(preds_model_defaults_f)):\n",
    "    if preds_model_defaults_f[i] < 0:\n",
    "        preds_model_defaults_f[i] = 0\n",
    "print(preds_model_defaults_f[preds_model_defaults_f<0])\n",
    "\n",
    "print(model_defaults_f.score(X_train_f, y_train_f), model_defaults_f.score(X_test_f, y_test_f))\n",
    "print(model_defaults_f.cv_results_['mean_test_score'])\n",
    "print(f\"{root_mean_squared_log_error(y_test_f, preds_model_defaults_f)} is the rmsle of {current_gender}\")\n",
    "print(f\"~\"*26)\n",
    "\n",
    "###########################################\n",
    "\n",
    "current_gender = \"indetermined\" #Oh, that's spelled with a U. Oh well.\n",
    "lr = LinearRegression()\n",
    "\n",
    "rfe = RFECV(lr, step=1, cv=5,\n",
    "                       min_features_to_select=5)\n",
    "\n",
    "model_defaults_i = rfe.fit(X_train_i, y_train_i)\n",
    "\n",
    "preds_model_defaults_i = model_defaults_i.predict(X_test_i)\n",
    "\n",
    "for i in range(len(preds_model_defaults_i)):\n",
    "    if preds_model_defaults_i[i] < 0:\n",
    "        preds_model_defaults_i[i] = 0\n",
    "print(preds_model_defaults_i[preds_model_defaults_i<0])\n",
    "\n",
    "print(model_defaults_i.score(X_train_i, y_train_i), model_defaults_i.score(X_test_i, y_test_i))\n",
    "print(model_defaults_i.cv_results_['mean_test_score'])\n",
    "print(f\"{root_mean_squared_log_error(y_test_i, preds_model_defaults_i)} is the rmsle of {current_gender}\")\n",
    "print(f\"~\"*26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e13c9",
   "metadata": {},
   "source": [
    "Hmm, happy with females too but i is now so much lower.... Hmm, was the score when we didn't segregate them so much highr\n",
    "because we were also scoring simultaneously males and females?\n",
    "\n",
    "Recall the 'baseeline' was .058...; gender proportions being:\n",
    "I    0.365184\n",
    "M    0.342405\n",
    "F    0.292410\n",
    "\n",
    "Still... something weird is going on and hopfully I just made a mistake somewhere in the the coding. Ie forget even quoting previous things - like at the local data - I has a much much much higher r2, yet its rmsle is much worse...\n",
    "\n",
    "We'll brute our way through forests too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4306617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0.9343130583564948 0.5491228686995352\n",
      "0.05067057900971129 is the rmsle of males\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[]\n",
      "0.9246620030425811 0.4711484328439606\n",
      "0.05263629846472247 is the rmsle of females\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[]\n",
      "0.9525678183393625 0.6578880832575142\n",
      "0.05945976064120366 is the rmsle of indetermined\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "#And now the moment of truth!\n",
    "\n",
    "current_gender = \"males\"\n",
    "\n",
    "rf = RandomForestRegressor(random_state=26)\n",
    "\n",
    "model_defaults_m = rf.fit(X_train_m, y_train_m)\n",
    "\n",
    "preds_model_defaults_m = model_defaults_m.predict(X_test_m)\n",
    "\n",
    "for i in range(len(preds_model_defaults_m)):\n",
    "    if preds_model_defaults_m[i] < 0:\n",
    "        preds_model_defaults_m[i] = 0\n",
    "print(preds_model_defaults_m[preds_model_defaults_m<0])\n",
    "\n",
    "print(model_defaults_m.score(X_train_m, y_train_m), model_defaults_m.score(X_test_m, y_test_m))\n",
    "print(f\"{root_mean_squared_log_error(y_test_m, preds_model_defaults_m)} is the rmsle of {current_gender}\")\n",
    "print(f\"~\"*26)\n",
    "\n",
    "###########################################\n",
    "\n",
    "current_gender = \"females\"\n",
    "\n",
    "rf = RandomForestRegressor(random_state=26)\n",
    "\n",
    "model_defaults_f = rf.fit(X_train_f, y_train_f)\n",
    "\n",
    "preds_model_defaults_f = model_defaults_f.predict(X_test_f)\n",
    "\n",
    "for i in range(len(preds_model_defaults_f)):\n",
    "    if preds_model_defaults_f[i] < 0:\n",
    "        preds_model_defaults_f[i] = 0\n",
    "print(preds_model_defaults_f[preds_model_defaults_f<0])\n",
    "\n",
    "print(model_defaults_f.score(X_train_f, y_train_f), model_defaults_f.score(X_test_f, y_test_f))\n",
    "print(f\"{root_mean_squared_log_error(y_test_f, preds_model_defaults_f)} is the rmsle of {current_gender}\")\n",
    "print(f\"~\"*26)\n",
    "\n",
    "###########################################\n",
    "\n",
    "current_gender = \"indetermined\" #Oh, that's spelled with a U. Oh well.\n",
    "\n",
    "rf = RandomForestRegressor(random_state=26)\n",
    "\n",
    "model_defaults_i = rf.fit(X_train_i, y_train_i)\n",
    "\n",
    "preds_model_defaults_i = model_defaults_i.predict(X_test_i)\n",
    "\n",
    "for i in range(len(preds_model_defaults_i)):\n",
    "    if preds_model_defaults_i[i] < 0:\n",
    "        preds_model_defaults_i[i] = 0\n",
    "print(preds_model_defaults_i[preds_model_defaults_i<0])\n",
    "\n",
    "print(model_defaults_i.score(X_train_i, y_train_i), model_defaults_i.score(X_test_i, y_test_i))\n",
    "print(f\"{root_mean_squared_log_error(y_test_i, preds_model_defaults_i)} is the rmsle of {current_gender}\")\n",
    "print(f\"~\"*26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9c2985",
   "metadata": {},
   "source": [
    "Better all around, but once again I'm just quite confused at the discrepencies. Likely this is where domain knowledge would be key..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
